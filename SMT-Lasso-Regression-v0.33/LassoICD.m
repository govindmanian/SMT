function [b]=LassoICD(X,y,lambda,K,option, E, Lambda, b_ini)%This function uses iterative coordinate descent method to find the optimal%"b" to minimize ||y-X*b||+\lambda * |b|_1, which is named as Lasso problem;%With the option "WithNormalization" the function behaves as a traditional %Lasso algorithm. With the option "SMTNormalization" the function behaves%as a SMT based lasso.%The description of the ICD algrothm can be found in the Laboratory Procedure %written by Prof. Bouman. %https://engineering.purdue.edu/~bouman/grad-labs/lab9/%The DRSS is the derivative of RSS, the description of which can be found in%the paper "Penalized regressions: the bridge vs the lasso" by Wenjiang Fu%The description of Lasso can be found in the original lasso paper%Tibshirani, R. (1996). Regression shrinkage and selection via the lasso.if lambda~=0                                                               %When lambda=0, we use pinv directly;   [n,P]=size(X);   eps=1e-5;   if strcmp(option,'WithNormalization');       mu=mean(X);       sigma2 = std(X);       ndx = find(sigma2 < eps);       sigma2(ndx) = 1;       E=zeros(P,P);       for i=1:P;            E(i,i)=1/sigma2(i);       end       X = X *E;       X(:,P)=ones(n,1);   elseif strcmp(option,'SMTNormalization');        %disp('SMT based');        if nargin<6;            [E,Lambda,SMTArray]=SMTCovarEst_withoutSORT(X');            SMTlambda=diag(Lambda);            SMTlambda_inv=SMTlambda.^(-1);            Xtilde=X;            [iterations_no,three]=size(SMTArray);            for iter=1:iterations_no;                Xi=Xtilde(:,SMTArray(iter,1))*cos(SMTArray(iter,3))+Xtilde(:,SMTArray(iter,2))*sin(SMTArray(iter,3));                Xj=-Xtilde(:,SMTArray(iter,1))*sin(SMTArray(iter,3))+Xtilde(:,SMTArray(iter,2))*cos(SMTArray(iter,3));                Xtilde(:,SMTArray(iter,1))=Xi;                Xtilde(:,SMTArray(iter,2))=Xj;            end            Xtilde=Xtilde*diag(SMTlambda_inv.^(1/2));            X=Xtilde;        else            SMTlambda=diag(Lambda);            SMTlambda_inv=SMTlambda.^(-1);            Xtilde=X;            Xtilde=Xtilde*E*diag(SMTlambda_inv.^(1/2));            X=Xtilde;            %b=E*diag(SMTlambda_inv.^(1/2))*b; %It seems that to use this multiply directly is even faster...        end   else       disp('Warning: the option has to be either "WithNormalization" or "SMTNormalization"');   end    %Initialize the regression vector beta;    if nargin<8;        b = (X'*X + lambda*eye(P))\(X'*y);    else        b=b_ini;    end    e=y-X*b;    %Precompute the Hessian diagonals    XX=X'*X;    for k=1:K;        b_pre=b;        for i=1:P;            v=b(i);            DRSS=-2*e'*X(:,i)-2*v*XX(i,i);            if DRSS>lambda;                u=(lambda-DRSS)/(2*XX(i,i));            elseif DRSS<-lambda;                u=(-lambda-DRSS)/(2*XX(i,i));            elseif abs(DRSS)<=lambda;                u=0;            end            b(i)=u;            e=e-(X(:,i)).*(b(i)-v);        end        if norm(abs(b-b_pre),1) < eps;            break;        end        %SS(k)=(y-X*b)'*(y-X*b)+lambda*norm(b,1);    end%    k%   lambda    if k==K;        disp('Warning: The iteration number is not large enough, please check/enlarge the value of K');    end    %disp('The no. of iterations');    %k  %Used to debug;    if strcmp(option,'WithNormalization');        b=E*b;    elseif strcmp(option,'SMTNormalization');        b=E*diag(SMTlambda_inv.^(1/2))*b; %It seems that to use this multiply directly is even faster...    else       disp('Warning: the option has to be either "WithNormalization" or "SMTNormalization"');    endelse    b=pinv(X)*y;end%figure;%CC=SS(1:k-1);%plot(CC);
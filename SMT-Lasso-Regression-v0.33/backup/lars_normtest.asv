function b_lars=lars_normtest(X,y,option);
[n,P]=size(X);
   eps=1e-5;
   if strcmp(option,'WithNormalization');
       mu=mean(X);
       sigma2 = std(X);
       ndx = find(sigma2 < eps);
       sigma2(ndx) = 1;
       E=zeros(P,P);
       for i=1:P;
            E(i,i)=1/sigma2(i);
       end
       X = X *E;
       X(:,P)=ones(n,1);
   elseif strcmp(option,'SMTNormalization');
        %disp('SMT based');
        [E,Lambda,SMTArray]=SMTCovarEst_withoutSORT(Xtr');
            SMTlambda=diag(Lambda);
            SMTlambda_inv=SMTlambda.^(-1);
            Xtilde=X;
            Xtilde=Xtilde*E*diag(SMTlambda_inv.^(1/2));
            X=Xtilde;
            %b=E*diag(SMTlambda_inv.^(1/2))*b; %It seems that to use this multiply directly is even faster...
   else
       disp('Warning: the option has to be either "WithNormalization" or "SMTNormalization"');
   end
    %Initialize the regression vector beta;

    b=lars(X)









X = diabetes.x;
X = normalize(X);
y = diabetes.y;
y = center(y);
[n p] = size(X);

b1 = lars(X, y, 'lasso', 0, 0, [], 1);
s1 = sum(abs(b1),2)/sum(abs(b1(end,:)));
[s_opt, b_opt, res_mean, res_std] = crossvalidate(@lars, 10, 1000, X, y, 'lasso', 0, 0, [], 0);
cvplot(s_opt, res_mean, res_std);

figure;
hold on;
plot(s1, b1, '.-');
axis tight;
ax = axis;
line([s_opt s_opt], [ax(3) ax(4)], 'Color', 'r', 'LineStyle', '-.');
legend
title('Least Angle Regression (LASSO)');